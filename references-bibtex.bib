
@inproceedings{bolin2005,
  title = {Automation and Customization of Rendered Web Pages},
  booktitle = {Proceedings of the 18th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology  - {{UIST}} '05},
  author = {Bolin, Michael and Webber, Matthew and Rha, Philip and Wilson, Tom and Miller, Robert C.},
  year = {2005},
  pages = {163},
  publisher = {{ACM Press}},
  address = {{Seattle, WA, USA}},
  doi = {10.1145/1095034.1095062},
  abstract = {On the desktop, an application can expect to control its user interface down to the last pixel, but on the World Wide Web, a content provider has no control over how the client will view the page, once delivered to the browser. This creates an opportunity for end-users who want to automate and customize their web experiences, but the growing complexity of web pages and standards prevents most users from realizing this opportunity. We describe Chickenfoot, a programming system embedded in the Firefox web browser, which enables end-users to automate, customize, and integrate web applications without examining their source code. One way Chickenfoot addresses this goal is a novel technique for identifying page components by keyword pattern matching. We motivate this technique by studying how users name web page components, and present a heuristic keyword matching algorithm that identifies the desired component from the user's name.},
  file = {/Users/geoffreylitt/Zotero/storage/5EF4RHD9/Bolin et al. - 2005 - Automation and customization of rendered web pages.pdf},
  isbn = {978-1-59593-271-6},
  language = {en}
}

@inproceedings{briot2016,
  title = {High {{Responsiveness}} for {{Group Editing CRDTs}}},
  booktitle = {Proceedings of the 19th {{International Conference}} on {{Supporting Group Work}}},
  author = {Briot, Lo{\"i}ck and Urso, Pascal and Shapiro, Marc},
  year = {2016},
  month = nov,
  pages = {51--60},
  publisher = {{ACM}},
  address = {{Sanibel Island Florida USA}},
  doi = {10.1145/2957276.2957300},
  abstract = {Group editing is a crucial feature for many end-user applications. It requires high responsiveness, which can be provided only by optimistic replication algorithms, which come in two classes: classical Operational Transformation (OT), or more recent Conflict-Free Replicated Data Types (CRDTs).},
  file = {/Users/geoffreylitt/Zotero/storage/PJUXMG6P/Briot et al. - 2016 - High Responsiveness for Group Editing CRDTs.pdf},
  isbn = {978-1-4503-4276-6},
  language = {en}
}

@inproceedings{chang2014b,
  title = {Creating Interactive Web Data Applications with Spreadsheets},
  booktitle = {Proceedings of the 27th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology},
  author = {Chang, Kerry Shih-Ping and Myers, Brad A.},
  year = {2014},
  month = oct,
  pages = {87--96},
  publisher = {{ACM}},
  address = {{Honolulu Hawaii USA}},
  doi = {10.1145/2642918.2647371},
  abstract = {While more and more data are available through web services, it remains difficult for end-users to create web applications that make use of these data without having to write complex code. We present Gneiss, a live programming environment that extends the spreadsheet metaphor to support creating interactive web applications that dynamically use local and web data from multiple sources. Gneiss closely integrates a spreadsheet editor with a web interface builder to let users demonstrate bindings between properties of web GUI elements and cells in the spreadsheet while working with real web service data. The spreadsheet editor provides two-way connections to web services, to both visualize and retrieve different data based on the user input in the web interface. Gneiss achieves rich interactivity without the need for event-based programming by extending the ``pull model'' of formulas that is familiar to the spreadsheet users. We use a series of examples to demonstrate Gneiss's ability to create a variety of interactive web data applications.},
  file = {/Users/geoffreylitt/Zotero/storage/VP2KVS7X/Chang and Myers - 2014 - Creating interactive web data applications with sp.pdf},
  isbn = {978-1-4503-3069-5},
  language = {en}
}

@inproceedings{chasins2018,
  title = {Rousillon: {{Scraping Distributed Hierarchical Web Data}}},
  shorttitle = {Rousillon},
  booktitle = {The 31st {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}} - {{UIST}} '18},
  author = {Chasins, Sarah E. and Mueller, Maria and Bodik, Rastislav},
  year = {2018},
  pages = {963--975},
  publisher = {{ACM Press}},
  address = {{Berlin, Germany}},
  doi = {10.1145/3242587.3242661},
  abstract = {Programming by Demonstration (PBD) promises to enable data scientists to collect web data. However, in formative interviews with social scientists, we learned that current PBD tools are insufficient for many real-world web scraping tasks. The missing piece is the capability to collect hierarchicallystructured data from across many different webpages. We present Rousillon, a programming system for writing complex web automation scripts by demonstration. Users demonstrate how to collect the first row of a `universal table' view of a hierarchical dataset to teach Rousillon how to collect all rows. To offer this new demonstration model, we developed novel relation selection and generalization algorithms. In a withinsubject user study on 15 computer scientists, users can write hierarchical web scrapers 8 times more quickly with Rousillon than with traditional programming.},
  file = {/Users/geoffreylitt/Zotero/storage/QUZNCYYS/Chasins et al. - 2018 - Rousillon Scraping Distributed Hierarchical Web D.pdf},
  isbn = {978-1-4503-5948-1},
  language = {en}
}

@inproceedings{drosos2020,
  title = {Wrex: {{A Unified Programming}}-by-{{Example Interaction}} for {{Synthesizing Readable Code}} for {{Data Scientists}}},
  shorttitle = {Wrex},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Drosos, Ian and Barik, Titus and Guo, Philip J. and DeLine, Robert and Gulwani, Sumit},
  year = {2020},
  month = apr,
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3313831.3376442},
  abstract = {Data wrangling is a difficult and time-consuming activity in computational notebooks, and existing wrangling tools do not fit the exploratory workflow for data scientists in these environments. We propose a unified interaction model based on programming-by-example that generates readable code for a variety of useful data transformations, implemented as a Jupyter notebook extension called Wrex. User study results demonstrate that data scientists are significantly more effective and efficient at data wrangling with Wrex over manual programming. Qualitative participant feedback indicates that Wrex was useful and reduced barriers in having to recall or look up the usage of various data transform functions. The synthesized code allowed data scientists to verify the intended data transformation, increased their trust and confidence in Wrex, and fit seamlessly within their cell-based notebook workflows. This work suggests that presenting readable code to professional data scientists is an indispensable component of offering data wrangling tools in notebooks.},
  file = {/Users/geoffreylitt/Zotero/storage/XJDAT57A/Drosos et al. - 2020 - Wrex A Unified Programming-by-Example Interaction.pdf},
  isbn = {978-1-4503-6708-0},
  keywords = {computational notebooks,data science,program synthesis},
  series = {{{CHI}} '20}
}

@inproceedings{drosos2020a,
  title = {Wrex: {{A Unified Programming}}-by-{{Example Interaction}} for {{Synthesizing Readable Code}} for {{Data Scientists}}},
  shorttitle = {Wrex},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Drosos, Ian and Barik, Titus and Guo, Philip J. and DeLine, Robert and Gulwani, Sumit},
  year = {2020},
  month = apr,
  pages = {1--12},
  publisher = {{ACM}},
  address = {{Honolulu HI USA}},
  doi = {10.1145/3313831.3376442},
  abstract = {Data wrangling is a diffcult and time-consuming activity in computational notebooks, and existing wrangling tools do not ft the exploratory workfow for data scientists in these environments. We propose a unifed interaction model based on programming-by-example that generates readable code for a variety of useful data transformations, implemented as a Jupyter notebook extension called WREX. User study results demonstrate that data scientists are signifcantly more effective and effcient at data wrangling with WREX over manual programming. Qualitative participant feedback indicates that WREX was useful and reduced barriers in having to recall or look up the usage of various data transform functions. The synthesized code allowed data scientists to verify the intended data transformation, increased their trust and confdence in WREX, and ft seamlessly within their cell-based notebook workfows. This work suggests that presenting readable code to professional data scientists is an indispensable component of offering data wrangling tools in notebooks.},
  file = {/Users/geoffreylitt/Zotero/storage/5RT8CTGB/Drosos et al. - 2020 - Wrex A Unified Programming-by-Example Interaction.pdf},
  isbn = {978-1-4503-6708-0},
  language = {en}
}

@inproceedings{furche2016,
  title = {Robust and {{Noise Resistant Wrapper Induction}}},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Furche, Tim and Guo, Jinsong and Maneth, Sebastian and Schallhart, Christian},
  year = {2016},
  month = jun,
  pages = {773--784},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2882903.2915214},
  abstract = {Wrapper induction is the problem of automatically inferring a query from annotated web pages of the same template. This query should not only select the annotated content accurately but also other content following the same template. Beyond accurately matching the template, we consider two additional requirements: (1) wrappers should be robust against a large class of changes to the web pages, and (2) the induction process should be noise resistant, i.e., tolerate slightly erroneous (e.g., machine generated) samples. Key to our approach is a query language that is powerful enough to permit accurate selection, but limited enough to force noisy samples to be generalized into wrappers that select the likely intended items. We introduce such a language as subset of XPATH and show that even for such a restricted language, inducing optimal queries according to a suitable scoring is infeasible. Nevertheless, our wrapper induction framework infers highly robust and noise resistant queries. We evaluate the queries on snapshots from web pages that change over time as provided by the Internet Archive, and show that the induced queries are as robust as the human-made queries. The queries often survive hundreds sometimes thousands of days, with many changes to the relative position of the selected nodes (including changes on template level). This is due to the few and discriminative anchor (intermediately selected) nodes of the generated queries. The queries are highly resistant against positive noise (up to 50\%) and negative noise (up to 20\%).},
  file = {/Users/geoffreylitt/Zotero/storage/FVEXPYA9/Furche et al. - 2016 - Robust and Noise Resistant Wrapper Induction.pdf},
  isbn = {978-1-4503-3531-7},
  keywords = {wrapper,wrapper induction,wrapper maintenance,XPath},
  series = {{{SIGMOD}} '16}
}

@inproceedings{gulwani2014,
  title = {{{NLyze}}: Interactive Programming by Natural Language for Spreadsheet Data Analysis and Manipulation},
  shorttitle = {{{NLyze}}},
  booktitle = {Proceedings of the 2014 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Gulwani, Sumit and Marron, Mark},
  year = {2014},
  month = jun,
  pages = {803--814},
  publisher = {{ACM}},
  address = {{Snowbird Utah USA}},
  doi = {10.1145/2588555.2612177},
  abstract = {Millions of computer end users need to perform tasks over tabular spreadsheet data, yet lack the programming knowledge to do such tasks automatically. This paper describes the design and implementation of a robust natural language based interface to spreadsheet programming. Our methodology involves designing a typed domain-specific language (DSL) that supports an expressive algebra of map, filter, reduce, join, and formatting capabilities at a level of abstraction appropriate for non-expert users. The key algorithmic component of our methodology is a translation algorithm for converting a natural language specification in the context of a given spreadsheet to a ranked set of likely programs in the DSL. The translation algorithm leverages the spreadsheet spatial and temporal context to assign interpretations to specifications with implicit references, and is thus robust to a variety of ways in which end users can express the same task. The translation algorithm builds over ideas from keyword programming and semantic parsing to achieve both high precision and high recall. We implemented the system as an Excel add-in called NLyze that supports a rich user interaction model including annotating the user's natural language specification and explaining the synthesized DSL programs by paraphrasing them into structured English. We collected a total of 3570 English descriptions for 40 spreadsheet tasks and our system was able to generate the intended interpretation as the top candidate for 94\% (97\% for the top 3) of those instances.},
  file = {/Users/geoffreylitt/Zotero/storage/KLSLS834/Gulwani and Marron - 2014 - NLyze interactive programming by natural language.pdf},
  isbn = {978-1-4503-2376-5},
  language = {en}
}

@inproceedings{hogue2005,
  title = {Thresher: Automating the Unwrapping of Semantic Content from the {{World Wide Web}}},
  shorttitle = {Thresher},
  booktitle = {Proceedings of the 14th International Conference on {{World Wide Web}}  - {{WWW}} '05},
  author = {Hogue, Andrew and Karger, David},
  year = {2005},
  pages = {86},
  publisher = {{ACM Press}},
  address = {{Chiba, Japan}},
  doi = {10.1145/1060745.1060762},
  abstract = {We describe Thresher, a system that lets non-technical users teach their browsers how to extract semantic web content from HTML documents on the World Wide Web. Users specify examples of semantic content by highlighting them in a web browser and describing their meaning. We then use the tree edit distance between the DOM subtrees of these examples to create a general pattern, or wrapper, for the content, and allow the user to bind RDF classes and predicates to the nodes of these wrappers. By overlaying matches to these patterns on standard documents inside the Haystack semantic web browser, we enable a rich semantic interaction with existing web pages, ``unwrapping'' semantic data buried in the pages' HTML. By allowing end-users to create, modify, and utilize their own patterns, we hope to speed adoption and use of the Semantic Web and its applications.},
  file = {/Users/geoffreylitt/Zotero/storage/PGCCZ8XN/Hogue and Karger - 2005 - Thresher automating the unwrapping of semantic co.pdf},
  isbn = {978-1-59593-046-0},
  language = {en}
}

@inproceedings{huynh2006,
  title = {Enabling Web Browsers to Augment Web Sites' Filtering and Sorting Functionalities},
  booktitle = {Proceedings of the 19th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology - {{UIST}} '06},
  author = {Huynh, David F. and Miller, Robert C. and Karger, David R.},
  year = {2006},
  pages = {125},
  publisher = {{ACM Press}},
  address = {{Montreux, Switzerland}},
  doi = {10.1145/1166253.1166274},
  abstract = {Existing augmentations of web pages are mostly small cosmetic changes (e.g., removing ads) and minor addition of third-party content (e.g., product prices from competing sites). None leverages the structured data presented in web pages. This paper describes Sifter, a web browser extension that can augment a well-structured web site with advanced filtering and sorting functionality. These added features work inside the site's own pages, preserving the site's presentational style and the user's context. Sifter contains an algorithm that scrapes structured data out of well-structured web pages while usually requiring no user intervention. We tested Sifter on real web sites and real users and found that people could use Sifter to perform sophisticated queries and high-level analyses on sizable data collections on the Web. We propose that web sites can be similarly augmented with other sophisticated data-centric functionality, giving users new benefits over the existing Web.},
  file = {/Users/geoffreylitt/Zotero/storage/ICV32DHL/Huynh et al. - 2006 - Enabling web browsers to augment web sites' filter.pdf},
  isbn = {978-1-59593-313-3},
  language = {en}
}


@inproceedings{kandel2011,
  title = {Wrangler: Interactive Visual Specification of Data Transformation Scripts},
  shorttitle = {Wrangler},
  booktitle = {Proceedings of the 2011 Annual Conference on {{Human}} Factors in Computing Systems - {{CHI}} '11},
  author = {Kandel, Sean and Paepcke, Andreas and Hellerstein, Joseph and Heer, Jeffrey},
  year = {2011},
  pages = {3363},
  publisher = {{ACM Press}},
  address = {{Vancouver, BC, Canada}},
  doi = {10.1145/1978942.1979444},
  abstract = {Though data analysis tools continue to improve, analysts still expend an inordinate amount of time and effort manipulating data and assessing data quality issues. Such ``data wrangling'' regularly involves reformatting data values or layout, correcting erroneous or missing values, and integrating multiple data sources. These transforms are often difficult to specify and difficult to reuse across analysis tasks, teams, and tools. In response, we introduce Wrangler, an interactive system for creating data transformations. Wrangler combines direct manipulation of visualized data with automatic inference of relevant transforms, enabling analysts to iteratively explore the space of applicable operations and preview their effects. Wrangler leverages semantic data types (e.g., geographic locations, dates, classification codes) to aid validation and type conversion. Interactive histories support review, refinement, and annotation of transformation scripts. User study results show that Wrangler significantly reduces specification time and promotes the use of robust, auditable transforms instead of manual editing.},
  file = {/Users/geoffreylitt/Zotero/storage/H6GQ7IEP/Kandel et al. - 2011 - Wrangler interactive visual specification of data.pdf},
  isbn = {978-1-4503-0228-9},
  language = {en}
}

@article{kushmerick2000,
  title = {Wrapper Induction: {{Efficiency}} and Expressiveness},
  shorttitle = {Wrapper Induction},
  author = {Kushmerick, Nicholas},
  year = {2000},
  month = apr,
  volume = {118},
  pages = {15--68},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(99)00100-9},
  abstract = {The Internet presents numerous sources of useful information\textemdash telephone directories, product catalogs, stock quotes, event listings, etc. Recently, many systems have been built that automatically gather and manipulate such information on a user's behalf. However, these resources are usually formatted for use by people (e.g., the relevant content is embedded in HTML pages), so extracting their content is difficult. Most systems use customized wrapper procedures to perform this extraction task. Unfortunately, writing wrappers is tedious and error-prone. As an alternative, we advocate wrapper induction, a technique for automatically constructing wrappers. In this article, we describe six wrapper classes, and use a combination of empirical and analytical techniques to evaluate the computational tradeoffs among them. We first consider expressiveness: how well the classes can handle actual Internet resources, and the extent to which wrappers in one class can mimic those in another. We then turn to efficiency: we measure the number of examples and time required to learn wrappers in each class, and we compare these results to PAC models of our task and asymptotic complexity analyses of our algorithms. Summarizing our results, we find that most of our wrapper classes are reasonably useful (70\% of surveyed sites can be handled in total), yet can rapidly learned (learning usually requires just a handful of examples and a fraction of a CPU second per example).},
  file = {/Users/geoffreylitt/Zotero/storage/EFQFKN2A/Kushmerick - 2000 - Wrapper induction Efficiency and expressiveness.pdf;/Users/geoffreylitt/Zotero/storage/STNVYBIX/S0004370299001009.html},
  journal = {Artificial Intelligence},
  keywords = {Information agents,Information extraction,Internet information integration,Machine learning,Wrapper induction},
  language = {en},
  number = {1}
}

@inproceedings{le2014,
  title = {{{FlashExtract}}: A Framework for Data Extraction by Examples},
  shorttitle = {{{FlashExtract}}},
  booktitle = {Proceedings of the 35th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Le, Vu and Gulwani, Sumit},
  year = {2014},
  month = jun,
  pages = {542--553},
  publisher = {{ACM}},
  address = {{Edinburgh United Kingdom}},
  doi = {10.1145/2594291.2594333},
  abstract = {Various document types that combine model and view (e.g., text files, webpages, spreadsheets) make it easy to organize (possibly hierarchical) data, but make it difficult to extract raw data for any further manipulation or querying. We present a general framework FlashExtract to extract relevant data from semi-structured documents using examples. It includes: (a) an interaction model that allows end-users to give examples to extract various fields and to relate them in a hierarchical organization using structure and sequence constructs. (b) an inductive synthesis algorithm to synthesize the intended program from few examples in any underlying domainspecific language for data extraction that has been built using our specified algebra of few core operators (map, filter, merge, and pair). We describe instantiation of our framework to three different domains: text files, webpages, and spreadsheets. On our benchmark comprising 75 documents, FlashExtract is able to extract intended data using an average of 2.36 examples in 0.84 seconds per field.},
  file = {/Users/geoffreylitt/Zotero/storage/W22WDEYS/Le and Gulwani - 2014 - FlashExtract a framework for data extraction by e.pdf},
  isbn = {978-1-4503-2784-8},
  language = {en}
}

@inproceedings{leshed2008,
  title = {{{CoScripter}}: Automating \&amp; Sharing How-to Knowledge in the Enterprise},
  shorttitle = {{{CoScripter}}},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Leshed, Gilly and Haber, Eben M. and Matthews, Tara and Lau, Tessa},
  year = {2008},
  month = apr,
  pages = {1719--1728},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1357054.1357323},
  abstract = {Modern enterprises are replete with numerous online processes. Many must be performed frequently and are tedious, while others are done less frequently yet are complex or hard to remember. We present interviews with knowledge workers that reveal a need for mechanisms to automate the execution of and to share knowledge about these processes. In response, we have developed the CoScripter system (formerly Koala [11]), a collaborative scripting environment for recording, automating, and sharing web-based processes. We have deployed CoScripter within a large corporation for more than 10 months. Through usage log analysis and interviews with users, we show that CoScripter has addressed many user automation and sharing needs, to the extent that more than 50 employees have voluntarily incorporated it into their work practice. We also present ways people have used CoScripter and general issues for tools that support automation and sharing of how-to knowledge.},
  file = {/Users/geoffreylitt/Zotero/storage/PF3HM38D/Leshed et al. - 2008 - CoScripter automating &amp\; sharing how-to knowle.pdf},
  isbn = {978-1-60558-011-1},
  keywords = {automation,knowledge sharing,procedural knowledge,programming-by-demonstration,scripting,user study,wiki},
  series = {{{CHI}} '08}
}

@inproceedings{lin2009,
  title = {End-User Programming of Mashups with Vegemite},
  booktitle = {Proceedings of the 14th International Conference on {{Intelligent}} User Interfaces},
  author = {Lin, James and Wong, Jeffrey and Nichols, Jeffrey and Cypher, Allen and Lau, Tessa A.},
  year = {2009},
  month = feb,
  pages = {97--106},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1502650.1502667},
  abstract = {Mashups are an increasingly popular way to integrate data from multiple web sites to fit a particular need, but it often requires substantial technical expertise to create them. To lower the barrier for creating mashups, we have extended the CoScripter web automation tool with a spreadsheet-like environment called Vegemite. Our system uses direct-manipulation and programming-by-demonstration tech-niques to automatically populate tables with information collected from various web sites. A particular strength of our approach is its ability to augment a data set with new values computed by a web site, such as determining the driving distance from a particular location to each of the addresses in a data set. An informal user study suggests that Vegemite may enable a wider class of users to address their information needs.},
  file = {/Users/geoffreylitt/Zotero/storage/2EP2P5ZS/Lin et al. - 2009 - End-user programming of mashups with vegemite.pdf},
  isbn = {978-1-60558-168-2},
  keywords = {automation,data integration,end-user programming,mashup,programming by demonstration,web},
  series = {{{IUI}} '09}
}

@inproceedings{litt2020,
  title = {Wildcard: {{Spreadsheet}}-{{Driven Customization}} of {{Web Applications}}},
  booktitle = {Companion {{Proceedings}} of the 4th {{International Conference}} on the {{Art}}, {{Science}}, and {{Engineering}} of {{Programming}}},
  author = {Litt, Geoffrey and Jackson, Daniel},
  year = {2020},
  pages = {10},
  publisher = {{Association for Computing Machinery}},
  address = {{Porto, Portugal.}},
  doi = {10.1145/3397537.3397541},
  abstract = {Many Web applications do not meet the precise needs of their users. Browser extensions offer a way to customize web applications, but most people do not have the programming skills to implement their own extensions.},
  file = {/Users/geoffreylitt/Zotero/storage/VJLCB4B6/Litt and Jackson - 2020 - Wildcard Spreadsheet-Driven Customization of Web .pdf},
  language = {en}
}

@inproceedings{litt2020a,
  title = {End-User {{Software Customization}} by {{Direct Manipulation}} of {{Tabular Data}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGPLAN International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} and {{Software}}},
  author = {Litt, Geoffrey and Jackson, Daniel and Millis, Tyler and Quaye, Jessica},
  year = {2020},
  month = nov,
  pages = {18--33},
  publisher = {{ACM}},
  address = {{Virtual USA}},
  doi = {10.1145/3426428.3426914},
  abstract = {Customizing software should be as easy as using it. Unfortunately, most customization methods require users to abruptly shift from using a graphical interface to writing scripts in a programming language.},
  file = {/Users/geoffreylitt/Zotero/storage/CXDRP7ZN/Litt et al. - 2020 - End-user software customization by direct manipula.pdf},
  isbn = {978-1-4503-8178-9},
  language = {en}
}

@inproceedings{litt2020b,
  title = {End-User Software Customization by Direct Manipulation of Tabular Data},
  booktitle = {Proceedings of the 2020 {{ACM SIGPLAN International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} and {{Software}}},
  author = {Litt, Geoffrey and Jackson, Daniel and Millis, Tyler and Quaye, Jessica},
  year = {2020},
  month = nov,
  pages = {18--33},
  publisher = {{ACM}},
  address = {{Virtual USA}},
  doi = {10.1145/3426428.3426914},
  abstract = {Customizing software should be as easy as using it. Unfortunately, most customization methods require users to abruptly shift from using a graphical interface to writing scripts in a programming language.},
  file = {/Users/geoffreylitt/Zotero/storage/K64V9RPF/Litt et al. - 2020 - End-user software customization by direct manipula.pdf},
  isbn = {978-1-4503-8178-9},
  language = {en}
}

@incollection{little2010,
  title = {Chapter 15 - {{Sloppy}} Programming},
  booktitle = {No {{Code Required}}},
  author = {Little, Greg and Miller, Robert C. and Chou, Victoria H. and Bernstein, Michael and Lau, Tessa and Cypher, Allen},
  editor = {Cypher, Allen and Dontcheva, Mira and Lau, Tessa and Nichols, Jeffrey},
  year = {2010},
  month = jan,
  pages = {289--307},
  publisher = {{Morgan Kaufmann}},
  address = {{Boston}},
  doi = {10.1016/B978-0-12-381541-5.00015-8},
  abstract = {The essence of sloppy programming is that the user should be able to enter something simple and natural, such as a few keywords, and the computer should try everything within its power to interpret and make sense of this input. This chapter discusses several prototypes that implement sloppy programming, translating sloppy commands directly into executable code. It also describes the algorithms used in these prototypes, exposes their limitations, and proposes directions for future work. The techniques described in this discussion still just scratch the surface of a domain with great potential: translating sloppy commands into executable code. It has described potential benefits to end users and expert programmers alike, as well as advocated a continued need for textual command interfaces. A number of prototypes are discussed exploring this technology and what one can learn from them, including the fact that users can form commands for some of these systems without any training. Finally, it gave some high-level technical details about how to go about actually implementing sloppy translation algorithms, with some references for future reading.},
  file = {/Users/geoffreylitt/Zotero/storage/DADYGABV/Little et al. - 2010 - Chapter 15 - Sloppy programming.pdf;/Users/geoffreylitt/Zotero/storage/IS4LIE9B/B9780123815415000158.html},
  isbn = {978-0-12-381541-5},
  language = {en}
}

@inproceedings{maclean1990,
  title = {User-Tailorable Systems: Pressing the Issues with Buttons},
  shorttitle = {User-Tailorable Systems},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {MacLean, Allan and Carter, Kathleen and L{\"o}vstrand, Lennart and Moran, Thomas},
  year = {1990},
  month = mar,
  pages = {175--182},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/97243.97271},
  abstract = {It is impossible to design systems which are appropriate for all users and all situations. We believe that a useful technique is to have end users tailor their systems to match their personal work practices. This requires not only systems which can be tailored, but a culture within which users feel in control of the system and in which tailoring is the norm. In a two-pronged research project we have worked closely with a group of users to develop a system to support tailoring and to help the users evolve a ``tailoring culture''. This has resulted in a flexible system based around the use of distributed on-screen Buttons to support a range of tailoring techniques.},
  file = {/Users/geoffreylitt/Zotero/storage/JD4VQANM/MacLean et al. - 1990 - User-tailorable systems pressing the issues with .pdf},
  isbn = {978-0-201-50932-8},
  series = {{{CHI}} '90}
}

@inproceedings{mayer2015,
  title = {User {{Interaction Models}} for {{Disambiguation}} in {{Programming}} by {{Example}}},
  booktitle = {Proceedings of the 28th {{Annual ACM Symposium}} on {{User Interface Software}} \& {{Technology}}},
  author = {Mayer, Mika{\"e}l and Soares, Gustavo and Grechkin, Maxim and Le, Vu and Marron, Mark and Polozov, Oleksandr and Singh, Rishabh and Zorn, Benjamin and Gulwani, Sumit},
  year = {2015},
  month = nov,
  pages = {291--301},
  publisher = {{ACM}},
  address = {{Charlotte NC USA}},
  doi = {10.1145/2807442.2807459},
  abstract = {Programming by Examples (PBE) has the potential to revo\- lutionize end-user programming by enabling end users, most of whom are non-programmers, to create small scripts for au\- tomating repetitive tasks. However, examples, though often easy to provide, are an ambiguous specification of the user's intent. Because of that, a key impedance in adoption of PBE systems is the lack of user confidence in the correctness of the program that was synthesized by the system. We present two novel user interaction models that communicate action\- able information to the user to help resolve ambiguity in the examples. One of these models allows the user to effectively navigate between the huge set of programs that are consis\- tent with the examples provided by the user. The other model uses active learning to ask directed example-based questions to the user on the test input data over which the user intends to run the synthesized program. Our user studies show that each of these models significantly reduces the number of errors in the performed task without any difference in completion time. Moreover, both models are perceived as useful, and the proactive active-learning based model has a slightly higher preference regarding the users' confidence in the result.},
  file = {/Users/geoffreylitt/Zotero/storage/QHTT6RAS/Mayer et al. - 2015 - User Interaction Models for Disambiguation in Prog.pdf},
  isbn = {978-1-4503-3779-3},
  language = {en}
}

@article{tanimoto1990,
  title = {{{VIVA}}: {{A}} Visual Language for Image Processing},
  shorttitle = {{{VIVA}}},
  author = {Tanimoto, Steven L.},
  year = {1990},
  month = jun,
  volume = {1},
  pages = {127--139},
  issn = {1045926X},
  doi = {10.1016/S1045-926X(05)80012-6},
  file = {/Users/geoffreylitt/Zotero/storage/ZLP5IM35/Tanimoto - 1990 - VIVA A visual language for image processing.pdf},
  journal = {Journal of Visual Languages \& Computing},
  language = {en},
  number = {2}
}

@inproceedings{tanimoto2013,
  title = {A Perspective on the Evolution of Live Programming},
  booktitle = {2013 1st {{International Workshop}} on {{Live Programming}} ({{LIVE}})},
  author = {Tanimoto, Steven L.},
  year = {2013},
  month = may,
  pages = {31--34},
  publisher = {{IEEE}},
  address = {{San Francisco, CA, USA}},
  doi = {10.1109/LIVE.2013.6617346},
  abstract = {Liveness in programming environments generally refers to the ability to modify a running program. Liveness is one form of a more general class of behaviors by a programming environment that provide information to programmers about what they are constructing. This paper gives a brief historical perspective on liveness and proposes an extension of a hierarchy given in 1990, to now account for even more powerful executionoriented tools for programmers. In addition, while liveness concerns the timeliness of execution feedback, considering a broader array of forms of feedback is helpful both in better understanding liveness and in designing ever more powerful development tools.},
  file = {/Users/geoffreylitt/Zotero/storage/UXPB7NGM/Tanimoto - 2013 - A perspective on the evolution of live programming.pdf},
  isbn = {978-1-4673-6265-8},
  language = {en}
}
